backend: vllm
# customization_cache_capacity: 10000
logging_level: INFO
# model_repo_path: /model-store
model_type: LLAMA3
pipeline:
# model_name: ensemble
# num_instances: 256
vllm:
  # ckpt_type: hf
  # data_type: float16
  # batch_size: 64
  # sequence_length: 128
  # model_path: /config/models/llama3_8b_pyt_safetensors_mode-pretrain
  # model_type: llama
  # num_gpus: 2
  pipeline-parallel-size: 1
  # quantize: null
  tensor-parallel-size: 2